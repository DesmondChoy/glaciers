reduce(bind_rows) %>%
select(page, text) %>%
unnest_tokens(sentence, text,
token = "sentences",
to_lower = FALSE) %>%
unnest_tokens(sentence, sentence,
token = "regex",
to_lower = FALSE,
pattern = "\\.|\\:|\\·|\\•") %>%
mutate(party = "PSP") %>%
select(sentence, page, party)
# Chunk 9: rdu
rdu_df <- pdf_data("rdu.pdf") %>%
imap(., ~.x %>% mutate(page = .y)) %>%
reduce(bind_rows) %>%
select(page, text) %>%
unnest_tokens(sentence, text,
token = "sentences",
to_lower = FALSE) %>%
unnest_tokens(sentence, sentence,
token = "regex",
to_lower = FALSE,
pattern = "\\.|\\:|\\·|\\•") %>%
mutate(party = "RDU") %>%
select(sentence, page, party)
# Chunk 10: rp spp nsp sda ppp
rp_df <- pdf_text("rp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "RP")
spp_df <- pdf_text("spp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "SPP")
nsp_df <- pdf_text("nsp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "NSP")
sda_df <- pdf_text("sda.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "SDA")
ppp_df <- pdf_text("ppp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "PPP")
# Chunk 11: combination
manifestos <- wp_df %>%
bind_rows(list(rp_df, spp_df, nsp_df, sda_df, ppp_df)) %>%
unnest_tokens(sentence, value,
token = "sentences",
to_lower = FALSE)
manif_df <- manifestos %>%
bind_rows(list(pap_df, sdp_df, psp_df, rdu_df))
manif_df
# Chunk 12: counting sentence
manif_df %>%
count(party, sort = TRUE)
# Chunk 13: categories
categories <- manif_df %>%
#transform all sentences to lower case
mutate(lower = str_to_lower(sentence)) %>%
#removing content pages
filter(!str_detect(lower, "(?i)content")) %>%
mutate(topic = case_when(
str_detect(lower, "\\bgst\\b|\\btax\\b|poverty|\\bpoor\\b|assistance|senior|\\belder|silver|retire|subsidy|subsidies|\\bage\\b|\\baged\\b|disabled|disabilities|\\bchild|\\bcost of living\\b|\\bsocial") ~ "Social Mobility",
str_detect(lower, "employ|\\bjob.?\\b|\\bwage|\\bwork\\b|\\bworker\\b|skillsfuture|retrench|labour|salary|\\bpmet") ~ "Labour",
str_detect(lower, "\\beconom|\\bsme\\b|\\brent\\b|growth|enterprise|\\bgdp\\b|business|\\bindust") ~ "Economy",
str_detect(lower, "\\bhdb|\\bflat|housing|\\blease|\\brental|\\bsers\\b|bloc\\b|cpf|payout|withdraw") ~ "CPF/Housing",
str_detect(lower, "education|class size|\\bmoe\\b|student|school|learning|kindergarten|universit") ~ "Education",
str_detect(lower, "constituencies|parliament|government|\\belection|\\bminist|president|independen|\\bvot|transparen|democracy|isa|\\binternal security") ~ "Governance",
str_detect(lower, "healthcare|\\bchas\\b|polyclinic|hospital|\\bdrug|patient|medisave|medishield|\\bmedic|insurance") ~ "Healthcare",
str_detect(lower, "\\blibert|gender|freedom|pofma|\\brights|\\bdiversity|assembly") ~ "Civil Liberties",
str_detect(lower, "climate|\\benergy|\\bgreen|\\bcarbon|\\bsolar|renewable|emissions|electric|\\bparis\\b|pollution") ~ "Climate Change",
str_detect(lower, "covid.?\\d+?|post-covid|disease|outbreak|pandemic") ~ "Covid-19",
TRUE ~ "Others")
) %>%
select(party, topic, sentence, page)
set.seed(123)
categories %>%
filter(!topic == "Others") %>%
sample_n(3) %>%
t()
# Chunk 14: english nuance
categories %>%
filter(str_detect(sentence, "below the age of 37")) %>%
t()
# Chunk 15: others
categories %>%
count(topic, sort = TRUE)
# Chunk 16: histogram
categories %>%
filter(topic == "Others") %>%
#count the number of letters in each sentence
mutate(length = str_count(sentence)) %>%
ggplot(aes(length)) +
geom_histogram(
binwidth = 20,
fill = fish(1, option = "Cephalopholis_argus", alpha = 0.6, begin = 0.2)
) +
labs(
x = "Number of Letters In Each Sentence",
y = "Number of Sentences",
title = "Analysing Sentences Categorized as \"Others\"",
subtitle = "A significant amount of sentences within Others contains ten letters or less"
) +
theme(plot.title = element_text(face = "bold", size = 20),
plot.subtitle = element_text(size = 17))
# Chunk 17: quantile
library(skimr)
categories %>%
filter(topic == "Others") %>%
#count the number of letters in each sentence
mutate(length = str_count(sentence)) %>%
skim_without_charts(length)
set.seed(2020)
categories %>%
filter(topic == "Others") %>%
mutate(length = str_count(sentence),
percentile = as.factor(case_when(length < 4 ~ "0-25",
between(length, 4, 50) ~ "26-50",
between(length, 51, 102) ~ "51-75",
TRUE ~ "76-100"))) %>%
group_by(percentile) %>%
sample_n(5) %>%
slice(1:5)
blogdown:::serve_site()
# Chunk 1: setup
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
fig.width = 12,
fig.height = 10)
setwd("C:\\Users\\Desmond\\Documents\\GitHub\\glaciers\\content\\post")
# Chunk 2: libraries
library(tidyverse)
library(tidytext)
library(pdftools)
library(fishualize)
theme_set(theme_minimal())
# Chunk 3: wp
#1 page per row
wp_df <- pdf_text("wp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "WP")
# Chunk 4: check wp
wp_df %>%
slice(sample(1:length(page), 1)) %>%
select(page, value) %>%
unnest_tokens(sentence, value,
token = "sentences",
to_lower = FALSE)
# Chunk 5: pap
pap_df <- pdf_data("pap.pdf") %>%
imap(., ~.x %>% mutate(page = .y)) %>%
reduce(bind_rows) %>%
select(page, text) %>%
unnest_tokens(sentence, text,
token = "sentences",
to_lower = FALSE) %>%
unnest_tokens(sentence, sentence,
token = "regex",
to_lower = FALSE,
pattern = "\\.|\\:|\\·|\\•") %>%
mutate(party = "PAP") %>%
select(sentence, page, party)
# Chunk 7: sdp
sdp_df <- pdf_data("sdp.pdf") %>%
imap(., ~.x %>% mutate(page = .y)) %>%
reduce(bind_rows) %>%
select(page, text) %>%
unnest_tokens(sentence, text,
token = "sentences",
to_lower = FALSE) %>%
unnest_tokens(sentence, sentence,
token = "regex",
to_lower = FALSE,
pattern = "\\.|\\:|\\·|\\•") %>%
mutate(party = "SDP") %>%
select(sentence, page, party)
# Chunk 8: psp
psp_df <- pdf_data("psp.pdf") %>%
imap(., ~.x %>% mutate(page = .y)) %>%
reduce(bind_rows) %>%
select(page, text) %>%
unnest_tokens(sentence, text,
token = "sentences",
to_lower = FALSE) %>%
unnest_tokens(sentence, sentence,
token = "regex",
to_lower = FALSE,
pattern = "\\.|\\:|\\·|\\•") %>%
mutate(party = "PSP") %>%
select(sentence, page, party)
# Chunk 9: rdu
rdu_df <- pdf_data("rdu.pdf") %>%
imap(., ~.x %>% mutate(page = .y)) %>%
reduce(bind_rows) %>%
select(page, text) %>%
unnest_tokens(sentence, text,
token = "sentences",
to_lower = FALSE) %>%
unnest_tokens(sentence, sentence,
token = "regex",
to_lower = FALSE,
pattern = "\\.|\\:|\\·|\\•") %>%
mutate(party = "RDU") %>%
select(sentence, page, party)
# Chunk 10: rp spp nsp sda ppp
rp_df <- pdf_text("rp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "RP")
spp_df <- pdf_text("spp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "SPP")
nsp_df <- pdf_text("nsp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "NSP")
sda_df <- pdf_text("sda.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "SDA")
ppp_df <- pdf_text("ppp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "PPP")
# Chunk 11: combination
manifestos <- wp_df %>%
bind_rows(list(rp_df, spp_df, nsp_df, sda_df, ppp_df)) %>%
unnest_tokens(sentence, value,
token = "sentences",
to_lower = FALSE)
manif_df <- manifestos %>%
bind_rows(list(pap_df, sdp_df, psp_df, rdu_df))
manif_df
categories <- manif_df %>%
#transform all sentences to lower case
mutate(lower = str_to_lower(sentence)) %>%
#removing content pages
filter(!str_detect(lower, "(?i)content")) %>%
mutate(topic = case_when(
str_detect(lower, "\\bgst\\b|\\btax\\b|poverty|\\bpoor\\b|assistance|senior|\\belder|silver|retire|subsidy|subsidies|\\bage\\b|\\baged\\b|disabled|disabilities|\\bchild|\\bcost of living\\b|\\bsocial") ~ "Social Mobility",
str_detect(lower, "employ|\\bjob.?\\b|\\bwage|\\bwork\\b|\\bworker\\b|skillsfuture|retrench|labour|salary|\\bpmet") ~ "Labour",
str_detect(lower, "\\beconom|\\bsme\\b|\\brent\\b|growth|enterprise|\\bgdp\\b|business|\\bindust") ~ "Economy",
str_detect(lower, "\\bhdb|\\bflat|housing|\\blease|\\brental|\\bsers\\b|bloc\\b|cpf|payout|withdraw") ~ "CPF/Housing",
str_detect(lower, "education|class size|\\bmoe\\b|student|school|learning|kindergarten|universit") ~ "Education",
str_detect(lower, "constituencies|parliament|government|\\belection|\\bminist|president|independen|\\bvot|transparen|democracy|isa|\\binternal security") ~ "Governance",
str_detect(lower, "healthcare|\\bchas\\b|polyclinic|hospital|\\bdrug|patient|medisave|medishield|\\bmedic|insurance") ~ "Healthcare",
str_detect(lower, "\\blibert|gender|freedom|pofma|\\brights|\\bdiversity|assembly") ~ "Civil Liberties",
str_detect(lower, "climate|\\benergy|\\bgreen|\\bcarbon|\\bsolar|renewable|emissions|electric|\\bparis\\b|pollution") ~ "Climate Change",
str_detect(lower, "covid.?\\d+?|post-covid|disease|outbreak|pandemic") ~ "Covid-19",
TRUE ~ "Others")
) %>%
select(party, topic, sentence, page)
set.seed(123)
categories %>%
filter(!topic == "Others") %>%
sample_n(3) %>%
t()
categories %>%
filter(str_detect(sentence, "below the age of 37")) %>%
t()
categories %>%
count(topic, sort = TRUE)
categories %>%
filter(topic == "Others") %>%
#count the number of letters in each sentence
mutate(length = str_count(sentence)) %>%
ggplot(aes(length)) +
geom_histogram(
binwidth = 20,
fill = fish(1, option = "Cephalopholis_argus", alpha = 0.6, begin = 0.2)
) +
labs(
x = "Number of Letters In Each Sentence",
y = "Number of Sentences",
title = "Analysing Sentences Categorized as \"Others\"",
subtitle = "A significant amount of sentences within Others contains ten letters or less"
) +
theme(plot.title = element_text(face = "bold", size = 20),
plot.subtitle = element_text(size = 17))
library(skimr)
categories %>%
filter(topic == "Others") %>%
#count the number of letters in each sentence
mutate(length = str_count(sentence)) %>%
skim_without_charts(length)
set.seed(2020)
categories %>%
filter(topic == "Others") %>%
mutate(length = str_count(sentence),
percentile = as.factor(case_when(length < 4 ~ "0-25",
between(length, 4, 50) ~ "26-50",
between(length, 51, 102) ~ "51-75",
TRUE ~ "76-100"))) %>%
group_by(percentile) %>%
sample_n(5) %>%
slice(1:5)
blogdown:::serve_site()
categories %>%
filter(topic == "Others") %>%
mutate(length = str_count(sentence),
percentile = as.factor(case_when(length < 4 ~ "0-25",
between(length, 4, 50) ~ "26-50",
between(length, 51, 102) ~ "51-75",
TRUE ~ "76-100"))) %>%
group_by(percentile) %>%
sample_n(5) %>%
slice(1:5)
set.seed(2020)
categories %>%
filter(topic == "Others") %>%
mutate(length = str_count(sentence),
percentile = as.factor(case_when(length < 4 ~ "0-25",
between(length, 4, 50) ~ "26-50",
between(length, 51, 102) ~ "51-75",
TRUE ~ "76-100"))) %>%
group_by(percentile) %>%
sample_n(5) %>%
slice(1:5)
blogdown:::serve_site()
pdf_data("pap.pdf") %>%
imap(., ~.x %>% mutate(page = .y)) %>%
reduce(bind_rows) %>%
select(page, text) %>%
unnest_tokens(sentence, text,
token = "sentences",
to_lower = FALSE) %>%
mutate(length = str_count(sentence)) %>%
arrange(desc(length))
# Chunk 1: setup
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
fig.width = 12,
fig.height = 10)
setwd("C:\\Users\\Desmond\\Documents\\GitHub\\glaciers\\content\\post")
# Chunk 2: libraries
library(tidyverse)
library(tidytext)
library(pdftools)
library(fishualize)
theme_set(theme_minimal())
# Chunk 3: wp
#1 page per row
wp_df <- pdf_text("wp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "WP")
# Chunk 4: check wp
wp_df %>%
slice(sample(1:length(page), 1)) %>%
select(page, value) %>%
unnest_tokens(sentence, value,
token = "sentences",
to_lower = FALSE)
# Chunk 5: pap
pap_df <- pdf_data("pap.pdf") %>%
imap(., ~.x %>% mutate(page = .y)) %>%
reduce(bind_rows) %>%
select(page, text) %>%
unnest_tokens(sentence, text,
token = "sentences",
to_lower = FALSE) %>%
unnest_tokens(sentence, sentence,
token = "regex",
to_lower = FALSE,
pattern = "\\.|\\:|\\·|\\•") %>%
mutate(party = "PAP") %>%
select(sentence, page, party)
# Chunk 7: sdp
sdp_df <- pdf_data("sdp.pdf") %>%
imap(., ~.x %>% mutate(page = .y)) %>%
reduce(bind_rows) %>%
select(page, text) %>%
unnest_tokens(sentence, text,
token = "sentences",
to_lower = FALSE) %>%
unnest_tokens(sentence, sentence,
token = "regex",
to_lower = FALSE,
pattern = "\\.|\\:|\\·|\\•") %>%
mutate(party = "SDP") %>%
select(sentence, page, party)
# Chunk 8: psp
psp_df <- pdf_data("psp.pdf") %>%
imap(., ~.x %>% mutate(page = .y)) %>%
reduce(bind_rows) %>%
select(page, text) %>%
unnest_tokens(sentence, text,
token = "sentences",
to_lower = FALSE) %>%
unnest_tokens(sentence, sentence,
token = "regex",
to_lower = FALSE,
pattern = "\\.|\\:|\\·|\\•") %>%
mutate(party = "PSP") %>%
select(sentence, page, party)
# Chunk 9: rdu
rdu_df <- pdf_data("rdu.pdf") %>%
imap(., ~.x %>% mutate(page = .y)) %>%
reduce(bind_rows) %>%
select(page, text) %>%
unnest_tokens(sentence, text,
token = "sentences",
to_lower = FALSE) %>%
unnest_tokens(sentence, sentence,
token = "regex",
to_lower = FALSE,
pattern = "\\.|\\:|\\·|\\•") %>%
mutate(party = "RDU") %>%
select(sentence, page, party)
# Chunk 10: rp spp nsp sda ppp
rp_df <- pdf_text("rp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "RP")
spp_df <- pdf_text("spp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "SPP")
nsp_df <- pdf_text("nsp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "NSP")
sda_df <- pdf_text("sda.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "SDA")
ppp_df <- pdf_text("ppp.pdf") %>%
as_tibble() %>%
mutate(page = row_number(),
party = "PPP")
# Chunk 11: combination
manifestos <- wp_df %>%
bind_rows(list(rp_df, spp_df, nsp_df, sda_df, ppp_df)) %>%
unnest_tokens(sentence, value,
token = "sentences",
to_lower = FALSE)
manif_df <- manifestos %>%
bind_rows(list(pap_df, sdp_df, psp_df, rdu_df))
manif_df
manif_df %>%
count(party, sort = TRUE)
categories <- manif_df %>%
#transform all sentences to lower case
mutate(lower = str_to_lower(sentence)) %>%
#removing content pages
filter(!str_detect(lower, "(?i)content")) %>%
mutate(topic = case_when(
str_detect(lower, "\\bgst\\b|\\btax\\b|poverty|\\bpoor\\b|assistance|senior|\\belder|silver|retire|subsidy|subsidies|\\bage\\b|\\baged\\b|disabled|disabilities|\\bchild|\\bcost of living\\b|\\bsocial") ~ "Social Mobility",
str_detect(lower, "employ|\\bjob.?\\b|\\bwage|\\bwork\\b|\\bworker\\b|skillsfuture|retrench|labour|salary|\\bpmet") ~ "Labour",
str_detect(lower, "\\beconom|\\bsme\\b|\\brent\\b|growth|enterprise|\\bgdp\\b|business|\\bindust") ~ "Economy",
str_detect(lower, "\\bhdb|\\bflat|housing|\\blease|\\brental|\\bsers\\b|bloc\\b|cpf|payout|withdraw") ~ "CPF/Housing",
str_detect(lower, "education|class size|\\bmoe\\b|student|school|learning|kindergarten|universit") ~ "Education",
str_detect(lower, "constituencies|parliament|government|\\belection|\\bminist|president|independen|\\bvot|transparen|democracy|isa|\\binternal security") ~ "Governance",
str_detect(lower, "healthcare|\\bchas\\b|polyclinic|hospital|\\bdrug|patient|medisave|medishield|\\bmedic|insurance") ~ "Healthcare",
str_detect(lower, "\\blibert|gender|freedom|pofma|\\brights|\\bdiversity|assembly") ~ "Civil Liberties",
str_detect(lower, "climate|\\benergy|\\bgreen|\\bcarbon|\\bsolar|renewable|emissions|electric|\\bparis\\b|pollution") ~ "Climate Change",
str_detect(lower, "covid.?\\d+?|post-covid|disease|outbreak|pandemic") ~ "Covid-19",
TRUE ~ "Others")
) %>%
select(party, topic, sentence, page)
set.seed(123)
categories %>%
filter(!topic == "Others") %>%
sample_n(3) %>%
t()
categories %>%
filter(str_detect(sentence, "below the age of 37")) %>%
t()
categories %>%
count(topic, sort = TRUE)
categories %>%
filter(topic == "Others") %>%
#count the number of letters in each sentence
mutate(length = str_count(sentence)) %>%
ggplot(aes(length)) +
geom_histogram(
binwidth = 20,
fill = fish(1, option = "Cephalopholis_argus", alpha = 0.6, begin = 0.2)
) +
labs(
x = "Number of Letters In Each Sentence",
y = "Number of Sentences",
title = "Analysing Sentences Categorized as \"Others\"",
subtitle = "A significant amount of sentences within Others contains ten letters or less"
) +
theme(plot.title = element_text(face = "bold", size = 20),
plot.subtitle = element_text(size = 17))
library(skimr)
categories %>%
filter(topic == "Others") %>%
#count the number of letters in each sentence
mutate(length = str_count(sentence)) %>%
skim_without_charts(length)
set.seed(2020)
categories %>%
filter(topic == "Others") %>%
mutate(length = str_count(sentence),
percentile = as.factor(case_when(length < 4 ~ "0-25",
between(length, 4, 50) ~ "26-50",
between(length, 51, 102) ~ "51-75",
TRUE ~ "76-100"))) %>%
group_by(percentile) %>%
sample_n(5) %>%
slice(1:5)
blogdown:::serve_site()
