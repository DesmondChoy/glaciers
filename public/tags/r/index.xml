<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>r on Desmond Choy&#39;s Blog</title>
    <link>/tags/r/</link>
    <description>Recent content in r on Desmond Choy&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 11 Apr 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Food Consumption and CO2 Emissions</title>
      <link>/2020/04/food-consumption-and-co2-emissions/</link>
      <pubDate>Sat, 11 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/food-consumption-and-co2-emissions/</guid>
      <description>This will be my first attempt at machine learning using the tidymodels package, with a dataset taken from TidyTuesday. The code used to scrape this data can be found here.
The study analyses data from the Food and Agriculture Organization of the United Nations (FAO) to determine the quantity of produce supplied for consumption of 11 food types for all countries researched. Using CO2 emissions data, the carbon footprint per capita is then calculated for each food type.</description>
    </item>
    
    <item>
      <title>Spotify Data? That&#39;s Music To My Ears!</title>
      <link>/2020/01/spotify-data-thats-music-to-my-ears/</link>
      <pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/01/spotify-data-thats-music-to-my-ears/</guid>
      <description>This dataset was taken from the very popular TidyTuesday github repo, and this was my attempt at having a go at visualization given my love for music and this was a Spotify dataset.
In the spirit of “Perfect is the enemy of good”, this will be a short post aimed at answering just a couple of questions with EDA and visualization.
Datasets from TidyTuesday are usually cleaned (or at least there’ll be instructions/hints on what one should first start with), and I begin by importing the data and exploring it via skimr.</description>
    </item>
    
    <item>
      <title>Oil and Gas Sector 2019 (Part II)</title>
      <link>/2019/12/oil-and-gas-sector-2019-part-ii/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/oil-and-gas-sector-2019-part-ii/</guid>
      <description>Continuing on from Part I of this analysis, we move on Part II.
Part I: Data Importing and Wrangling (Tidying)Part II: Exploratory Data Analysis and interpretation of resultsIf you wish to replicate my findings, the raw Bloomberg data is available at my github.
Disclaimer
All data that I have used in this presentation is obtained from Bloomberg and I do not own any of it.</description>
    </item>
    
    <item>
      <title>Oil and Gas Sector 2019 (Part I)</title>
      <link>/2019/12/oil-and-gas-sector-2019-part-i/</link>
      <pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/oil-and-gas-sector-2019-part-i/</guid>
      <description>While preparing for an internal sector presentation, I decided to experiment with R to see if I could replicate the results generated by Bloomberg and customize it further to shed more insight. The learning curve was steep but, on hindsight, I benefitted tremendously from the following:
A great opportunity to practise all aspects of Data Science. From Importing, to Tidying, to Exploratory Data Analysis, to my first ever R Markdown document!</description>
    </item>
    
  </channel>
</rss>